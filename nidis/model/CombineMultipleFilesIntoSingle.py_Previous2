from __future__ import division

import os
import logging
import rioxarray
import numpy as np
import xarray as xr
import geopandas as gpd
from netCDF4 import Dataset
from datetime import datetime, timedelta
from nidis.model.Metadata import \
    DictofNumNamePairs_Channels, DictofInitialToWord_Seasons



"""

# multithreaded array filling with thread pool and one worker per logical cpu
from time import time
from multiprocessing.pool import ThreadPool
from numpy import empty
 
# fill a portion of a larger array with a value
def fill_subarray(coords, data, value):
    # unpack array indexes
    i1, i2, i3, i4 = coords
    # populate subarray
    data[i1:i2,i3:i4].fill(value)
 
# task function
def task(n=50000):
    # record the start time
    start = time()
    # create an empty array
    data = empty((n,n))
    # create the thread pool
    with ThreadPool(8) as pool:
        # split each dimension (divisor of matrix dimension)
        split = round(n/4)
        # issue tasks
        for x in range(0, n, split):
            for y in range(0, n, split):
                # determine matrix coordinates
                coords = (x, x+split, y, y+split)
                # issue task
                _ = pool.apply_async(fill_subarray, args=(coords, data, 1))
        # close the pool
        pool.close()
        # wait for tasks to complete
        pool.join()
    # calculate and report duration
    duration = time() - start
    # return duration
    return duration
 
# experiment that averages duration of task function
def experiment(repeats=3):
    # repeat the experiment and gather results
    results = [task() for _ in range(repeats)]
    # return the average of the results
    return sum(results) / repeats
 
# run the experiment and report the result
duration = experiment()
print(f'Took {duration:.3f} seconds')
"""


def CombineMultipleFilesIntoSingle(
            indicator,
            season,
            indicator_dir,
            output_dir,
            n_pixels=469758
        ):

    if indicator > 0:

        output_filename = os.path.join(
            output_dir,
            f'NN_U_C_0To{str(int(round(float(n_pixels - 1))))}_In113_' +
            f'{DictofNumNamePairs_Channels[indicator]}_{season}.npz'
        )

    else: # of if indicator > 0:

        output_filename = os.path.join(
            output_dir,
            f'NN_U_C_0To{str(int(round(float(n_pixels - 1))))}_' +
            f'M{-1*indicator}_{season}.npz'
        )

    #end of if indicator > 0:

    if os.path.exists(output_filename):
        logging.info(f'Combined file exists, skipping {output_filename}')
        return

    FI1X1_ClmGrd1D_V2b_New_Array = np.empty((n_pixels,))
    FI1X1_ClmGrd1D_V2b_New_Array[:] = np.NaN

    SSiz1X1_ClmGrd1D_V2b_New_Array = np.empty((n_pixels,))
    SSiz1X1_ClmGrd1D_V2b_New_Array[:] = np.NaN

    WSiz1X1_ClmGrd1D_V2b_New_Array = np.empty((n_pixels,))
    WSiz1X1_ClmGrd1D_V2b_New_Array[:] = np.NaN

    # TODO: currently running in serial, I would like to make it parallel
    # End time:  406.6939799785614
    for WhichElem in range(n_pixels):

        if indicator > 0:
    
            indicator_result = np.loadtxt(
                os.path.join(
                    indicator_dir,
                    f'NN_U_C_{WhichElem}_In113_{indicator - 1}_{season}.txt'
                )
            )

        else: # of if indicator > 0:

            indicator_result = np.loadtxt(
                os.path.join(
                    indicator_dir,
                    f'NN_U_C_{WhichElem}_InM{-1*indicator}_0_{season}.txt'
                )
            )

        #end of if indicator > 0:

        FI1X1_ClmGrd1D_V2b_New_Array[WhichElem] = indicator_result[0]
        SSiz1X1_ClmGrd1D_V2b_New_Array[WhichElem] = indicator_result[1]
        WSiz1X1_ClmGrd1D_V2b_New_Array[WhichElem] = indicator_result[2]

    np.savez_compressed(
        output_filename,
        ArrayForSingleFile=FI1X1_ClmGrd1D_V2b_New_Array,
        FI1X1_ClmGrd1D_V2b_New_Array=FI1X1_ClmGrd1D_V2b_New_Array,
        SSiz1X1_ClmGrd1D_V2b_New_Array=SSiz1X1_ClmGrd1D_V2b_New_Array,
        WSiz1X1_ClmGrd1D_V2b_New_Array=WSiz1X1_ClmGrd1D_V2b_New_Array
    )
    logging.info(f'Saved output to {output_filename}')

    return output_filename


def ArrayToNetCDF(
            indicator,
            season,
            combined_output_dir,
            netcdf_filename,
            n_pixels=469758,
            nClimGrid_NC_File='/discover/nobackup/syatheen/Sujay/DeepLearning/Data/ML_Testcases/Drought_USDM/nClimGrid/monthly_spei_gamma/nclimgrid-spei-gamma-01.nc',
            ClimGrid1DShpFile='/discover/nobackup/syatheen/Sujay/DeepLearning/Data/ML_Testcases/Drought_USDM/nClimGrid/nClimGrid_as_poly_NoNans.shp',
            ncfile_title_EndingSubstr=' Pixel-specific Fractional Info of USDM (2006-01-03 To 2019-12-31)'
        ):

    if os.path.exists(netcdf_filename):
        logging.info(f'NetCDF file exists, skipping {netcdf_filename}')
        return

    if indicator > 0:
    
        # consider replacing with output from previous function
        combined_indicator_filename = os.path.join(
            combined_output_dir,
            f'NN_U_C_0To{str(int(round(float(n_pixels - 1))))}_In113_' +
            f'{DictofNumNamePairs_Channels[indicator]}_{season}.npz'
        )

    else: # of if indicator > 0:

        # consider replacing with output from previous function
        combined_indicator_filename = os.path.join(
            combined_output_dir,
            f'NN_U_C_0To{str(int(round(float(n_pixels - 1))))}_' +
            f'M{-1*indicator}_{season}.npz'
        )

    #end of if indicator > 0:

    # open nClimGrid_All
    nClimGrid_All = xr.open_dataset(nClimGrid_NC_File)

    lats = nClimGrid_All['lat'].values
    lons = nClimGrid_All['lon'].values

    FI_InNpz = np.load(combined_indicator_filename)
    FracIs = FI_InNpz['ArrayForSingleFile']
    Idxs = np.where((~np.isnan(FracIs)) & (FracIs > 1.0))
    FracIs[Idxs] = 1.0
    Idxs = np.where((~np.isnan(FracIs)) & (FracIs < 0.0))
    FracIs[Idxs] = 0.0

    SampleSizes = FI_InNpz['SSiz1X1_ClmGrd1D_V2b_New_Array']
    WindowSizes = FI_InNpz['WSiz1X1_ClmGrd1D_V2b_New_Array']

    ClimGrid1DShp = gpd.read_file(ClimGrid1DShpFile)
    PxlRowCol_SortedList_FrmShpFile = sorted(
        ClimGrid1DShp.PxlRowCol.values.tolist())
    PxlRowCols = np.array(PxlRowCol_SortedList_FrmShpFile)
    PxlRows = (np.around(PxlRowCols // 10000)).astype(int)
    PxlCols = (np.around(PxlRowCols % 10000)).astype(int)

    # create 2D arrays
    FI_Arr2D = np.empty((len(lats), len(lons)))
    FI_Arr2D[:] = np.NaN
    FI_Arr2D[PxlRows, PxlCols] = FracIs
    SSiz_Arr2D = np.empty((len(lats), len(lons)))
    SSiz_Arr2D[:] = np.NaN
    SSiz_Arr2D[PxlRows, PxlCols] = SampleSizes
    WSiz_Arr2D = np.empty((len(lats), len(lons)))
    WSiz_Arr2D[:] = np.NaN
    WSiz_Arr2D[PxlRows, PxlCols] = WindowSizes

    # just to be safe, make sure dataset is not already open.
    try:
        netcdf_filename.close()
    except:
        pass

    logging.info(f'Writing to {netcdf_filename}')
    ncfile = Dataset(netcdf_filename, mode='w', format='NETCDF4_CLASSIC')

    lat_dim = ncfile.createDimension('lat', len(lats))  # latitude axis
    lon_dim = ncfile.createDimension('lon', len(lons))  # longitude axis

    if indicator > 0:
    
        ncfile.title = \
            DictofNumNamePairs_Channels[indicator] + ' ' + \
            DictofInitialToWord_Seasons[season] + ncfile_title_EndingSubstr

    else: # of if indicator > 0:

        if indicator == -10:
            MultiIndicatorName = 'Remotely_sensed_multi-indicator'
        elif indicator == -12:
            MultiIndicatorName = 'Modeled_and_obsPrecip_multi-indicator'

        ncfile.title = \
            MultiIndicatorName + ' ' + \
            DictofInitialToWord_Seasons[season] + ncfile_title_EndingSubstr

    #end of if indicator > 0:

    # Define two variables with the same names as dimensions,
    # a conventional way to define "coordinate variables".
    lat = ncfile.createVariable('lat', np.float32, ('lat',))
    lat.units = 'degrees_north'
    lat.long_name = 'latitude'
    lon = ncfile.createVariable('lon', np.float32, ('lon',))
    lon.units = 'degrees_east'
    lon.long_name = 'longitude'

    # Define 2D variables to hold the data
    FracI = ncfile.createVariable(
        'FracI', np.float64, ('lat', 'lon'))
    FracI.units = '-'
    SampleSize = ncfile.createVariable(
        'SampleSize', np.float64, ('lat', 'lon'))
    SampleSize.units = '-'
    WindowSize = ncfile.createVariable(
        'WindowSize', np.float64, ('lat', 'lon'))
    WindowSize.units = '-'

    # Write latitudes, longitudes.
    # Note: the ":" is necessary in these "write" statements
    lat[:] = lats  # south to north
    lon[:] = lons  # Greenwich meridian eastward

    # Write the data.  This writes the whole 2D netCDF variable all at once.
    FracI[:, :] = FI_Arr2D
    SampleSize[:, :] = SSiz_Arr2D
    WindowSize[:, :] = WSiz_Arr2D

    # first print the Dataset object to see what we've got
    logging.info(ncfile)

    # close the Dataset.
    ncfile.close()
    logging.info(f'Dataset is saved and closed under {netcdf_filename}')

    return
